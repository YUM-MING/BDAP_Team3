# app.py
import streamlit as st
# --- Streamlit ë²„ì „ í™•ì¸ ì½”ë“œ (ë””ë²„ê¹…ìš©) ---
# st.write(f"í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” Streamlit ë²„ì „: {st.__version__}") # ì•± ì‹¤í–‰ ì‹œ ë¸Œë¼ìš°ì €ì— ë²„ì „ í‘œì‹œ
# --------------------------------------------

import pandas as pd
from collections import Counter
# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ import
import SNS.config as config # ìƒìˆ˜ ë° ì„¤ì •
import SNS.kote_module as kote_module
import SNS.youtube_api_module as youtube_api_module
import SNS.text_analysis_module as text_analysis_module
import SNS.ui_helpers as ui_helpers# UI í—¬í¼ ëª¨ë“ˆ (ì„ íƒì )

def run_sns():
    # KOTE ëª¨ë¸ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆ)
    kote_model = kote_module.load_trained_kote_model() # show_message=TrueëŠ” ê¸°ë³¸ê°’

    # YouTube API í‚¤ í™•ì¸
    if not youtube_api_module.YOUTUBE_API_KEY_VALUE:
        st.error("YouTube API í‚¤ê°€ .streamlit/secrets.toml íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # --- Streamlit ì•± UI êµ¬ì„± ---
    st.title("YouTube ì¬ë‚œ ì˜ìƒ ëŒ“ê¸€ ë¶„ì„ê¸°")
    st.markdown(
        "YouTube ì˜ìƒ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì—¬ ì¬ë‚œ ìœ í˜•ì„ ë¶„ë¥˜í•˜ê³ , KOTE ê°ì„± ë¶„ì„ ëª¨ë¸ì„ í†µí•´ ëŒ“ê¸€ì— ë‚˜íƒ€ë‚œ ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤. "
    )
    st.markdown("---")

    # --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
    if 'search_results' not in st.session_state:
        st.session_state.search_results = []
    if 'selected_video_ids_titles' not in st.session_state:
        st.session_state.selected_video_ids_titles = {}
    if 'all_comments_df' not in st.session_state:
        st.session_state.all_comments_df = pd.DataFrame()
    if 'main_search_button_clicked' not in st.session_state:
        st.session_state.main_search_button_clicked = False
    if 'main_analyze_button_clicked' not in st.session_state:
        st.session_state.main_analyze_button_clicked = False


    def is_disaster_related(query):
        """
        ì…ë ¥ëœ ê²€ìƒ‰ì–´ê°€ ì¬ë‚œ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        """
        # 1. ëª¨ë“  ë™ì˜ì–´ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
        all_disaster_keywords = [
            keyword 
            for keywords_list in config.DISASTER_SYNONYMS.values() 
            for keyword in keywords_list
        ]
        
        # 2. ê²€ìƒ‰ì–´ì— í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        #    any() í•¨ìˆ˜ëŠ” í•˜ë‚˜ë¼ë„ Trueì´ë©´ ì¦‰ì‹œ Trueë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ íš¨ìœ¨ì ì…ë‹ˆë‹¤.
        if any(keyword in query for keyword in all_disaster_keywords):
            return True
        return False
        

    # --- 1. ì˜ìƒ ê²€ìƒ‰ ë° ë¶„ì„ ì„¤ì • ì„¹ì…˜ ---
    with st.container():
        st.subheader("1. YouTube ì˜ìƒ ê²€ìƒ‰ ë° ë¶„ì„ ì„¤ì •")

        search_query = st.text_input(
            "ê²€ìƒ‰ì–´ ì…ë ¥:",
            placeholder="ì˜ˆ: í¬í•­ ì§€ì§„ í”¼í•´, ì„œìš¸ í­ìš° ìƒí™©",
            key="main_search_query"
        )

        col1_search_opts, col2_search_opts = st.columns(2)
        with col1_search_opts:
            sort_order_options = {
                "ê´€ë ¨ì„± ë†’ì€ ìˆœ": "relevance",
                "ì¡°íšŒìˆ˜ ë§ì€ ìˆœ": "viewCount",
            }
            sort_order_display = st.selectbox(
                "ì •ë ¬ ë°©ì‹:",
                list(sort_order_options.keys()),
                key="main_sort_order"
            )
            sort_order = sort_order_options[sort_order_display]
        with col2_search_opts:
            max_search_results = st.slider(
                "ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ìµœëŒ€):", 5, 50,
                config.DEFAULT_MAX_SEARCH_RESULTS,
                key="main_max_search_results"
            )

        if st.button("YouTube ì˜ìƒ ê²€ìƒ‰", type="primary", key="main_search_button_trigger_v2"):
            st.session_state.main_search_button_clicked = True
            
            # 1. ê²€ìƒ‰ì–´ ì…ë ¥ ì—¬ë¶€ í™•ì¸
            if not search_query:
                st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                # ì´ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ë‚¨ì•„ìˆì§€ ì•Šë„ë¡ ì´ˆê¸°í™”
                st.session_state.search_results = None 
            
            # 2. ì¬ë‚œ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ê°€ì¥ ì¤‘ìš”í•œ ë³€ê²½ì )
            elif not is_disaster_related(search_query):
                st.warning("ì¬ë‚œ ê´€ë ¨ í‚¤ì›Œë“œ(ì˜ˆ: ì§€ì§„, í™ìˆ˜, íƒœí’ ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ê²€ìƒ‰í•´ì£¼ì„¸ìš”.")
                # ìœ íš¨í•˜ì§€ ì•Šì€ ê²€ìƒ‰ì´ë¯€ë¡œ ì´ì „ ê²°ê³¼ ì´ˆê¸°í™”
                st.session_state.search_results = None
                
            # 3. ëª¨ë“  ê²€ì¦ í†µê³¼ ì‹œ, ì‹¤ì œ ê²€ìƒ‰ ì‹¤í–‰
            else:
                with st.spinner("YouTube ì˜ìƒì„ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
                    st.session_state.search_results = youtube_api_module.search_youtube_videos(
                        search_query, sort_order, max_search_results
                    )
                if not st.session_state.search_results:
                    st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                else:
                    st.session_state.selected_video_ids_titles = {}
                    st.session_state.all_comments_df = pd.DataFrame()
                    st.info(f"{len(st.session_state.search_results)}ê°œì˜ ì˜ìƒì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ë¶„ì„í•  ì˜ìƒì„ ì„ íƒí•˜ì„¸ìš”.")

        if st.session_state.search_results:
            st.markdown("##### ë¶„ì„í•  ì˜ìƒ ì„ íƒ (ìµœëŒ€ 5ê°œ)")
            video_options_dict = {
                f"{idx+1}. {v['title']} (ê²Œì‹œì¼: {v['published_at'][:10] if v['published_at'] else 'ë‚ ì§œ ì •ë³´ ì—†ìŒ'})": v['id']
                for idx, v in enumerate(st.session_state.search_results)
            }
            default_selected_display_keys = [
                display_key for display_key, vid_id in video_options_dict.items()
                if vid_id in st.session_state.selected_video_ids_titles.keys()
            ]

            selected_display_keys = st.multiselect(
                "ì•„ë˜ ëª©ë¡ì—ì„œ ë¶„ì„í•  ì˜ìƒì„ ì„ íƒí•˜ì„¸ìš”:",
                options=list(video_options_dict.keys()),
                max_selections=5,
                default=default_selected_display_keys,
                key="main_video_multiselect_v2" # í‚¤ ë³€ê²½
            )

            current_selected_temp = {}
            for display_key in selected_display_keys:
                video_id = video_options_dict.get(display_key)
                if video_id:
                    original_video_info = next((v for v in st.session_state.search_results if v['id'] == video_id), None)
                    if original_video_info:
                        current_selected_temp[video_id] = original_video_info['title']
            st.session_state.selected_video_ids_titles = current_selected_temp

            if st.session_state.selected_video_ids_titles:
                st.caption(f"ì„ íƒëœ ì˜ìƒ ìˆ˜: {len(st.session_state.selected_video_ids_titles)}ê°œ")
                cols_thumbnails = st.columns(min(len(st.session_state.selected_video_ids_titles), 5))
                search_results_map_by_id = {v['id']: v for v in st.session_state.search_results}
                for i, video_id in enumerate(list(st.session_state.selected_video_ids_titles.keys())[:5]):
                    video_info = search_results_map_by_id.get(video_id)
                    if video_info and video_info.get('thumbnail'):
                        with cols_thumbnails[i]:
                            st.image(video_info['thumbnail'], caption=video_info['title'][:30]+"...", width=150)
                            st.markdown(f"[{video_info['title'][:20]}...](https://www.youtube.com/watch?v={video_id})", unsafe_allow_html=True)

        st.markdown("---")
        st.subheader("2. ëŒ“ê¸€ ë¶„ì„ ì˜µì…˜ ì„¤ì •")
        col1_analysis_opts, col2_analysis_opts = st.columns(2)
        with col1_analysis_opts:
            max_comments_per_video = st.slider(
                "ì˜ìƒë³„ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ì§‘:", 50, 500,
                config.DEFAULT_MAX_COMMENTS_PER_VIDEO,
                step=50,
                key="main_max_comments_per_video_v2" # í‚¤ ë³€ê²½
            )
        with col2_analysis_opts:
            emotion_threshold = st.slider(
                "ê°ì • ë¶„ë¥˜ ì„ê³„ê°’ (KOTE):", 0.1, 0.9,
                config.DEFAULT_EMOTION_THRESHOLD,
                step=0.05,
                key="main_emotion_threshold_v2" # í‚¤ ë³€ê²½
            )

        if st.button(
            "ì„ íƒëœ ì˜ìƒ ëŒ“ê¸€ ë¶„ì„ ì‹œì‘",
            type="primary",
            disabled=not st.session_state.selected_video_ids_titles or not kote_model,
            key="main_analyze_button_trigger_v2" # í‚¤ ë³€ê²½
        ):
            st.session_state.main_analyze_button_clicked = True
            if not kote_model:
                st.error("KOTE ê°ì„± ë¶„ì„ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            elif st.session_state.selected_video_ids_titles:
                all_comments_list_for_df = []
                total_videos_to_analyze = len(st.session_state.selected_video_ids_titles)
                analysis_progress_bar = st.progress(0, text="ë¶„ì„ ì¤€ë¹„ ì¤‘...")
                analysis_status_text = st.empty()

                with st.spinner("ì„ íƒëœ ì˜ìƒì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
                    for i, (video_id, video_title) in enumerate(st.session_state.selected_video_ids_titles.items()):
                        progress_value_collect = int(((i + 0.5) / total_videos_to_analyze) * 80)
                        analysis_status_text.info(f"'{video_title}' ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘... ({i+1}/{total_videos_to_analyze})")
                        analysis_progress_bar.progress(progress_value_collect, text=f"ëŒ“ê¸€ ìˆ˜ì§‘: {video_title} ({i+1}/{total_videos_to_analyze})")

                        comments = youtube_api_module.get_video_comments(video_id, total_max_comments=max_comments_per_video)
                        for comment_data in comments:
                            comment_data['video_title'] = video_title
                        all_comments_list_for_df.extend(comments)

                    if not all_comments_list_for_df:
                        st.warning("ìˆ˜ì§‘ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ì˜ìƒ ì„ íƒ ë˜ëŠ” ëŒ“ê¸€ ìˆ˜ì§‘ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        st.session_state.all_comments_df = pd.DataFrame()
                        if analysis_progress_bar is not None: analysis_progress_bar.empty()
                        if analysis_status_text is not None: analysis_status_text.empty()
                    else:
                        analysis_status_text.info("ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ê°ì •/ì¬ë‚œ ë¼ë²¨ë§ ì¤‘...")
                        analysis_progress_bar.progress(80, text="í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘...")

                        df_raw_comments = pd.DataFrame(all_comments_list_for_df)

                        if 'text' in df_raw_comments.columns and not df_raw_comments['text'].isnull().all():
                            df_raw_comments['text'] = df_raw_comments['text'].astype(str)
                            comment_texts_list = df_raw_comments["text"].tolist()

                            df_raw_comments["disaster_labels"] = df_raw_comments["text"].apply(text_analysis_module.label_disaster)
                            df_raw_comments["sentiment_labels"] = kote_module.analyze_sentiment_kote_batch(
                                comment_texts_list, kote_model, emotion_threshold
                            )

                            df_raw_comments["published_at"] = pd.to_datetime(df_raw_comments["published_at"], errors='coerce')
                            df_raw_comments["comment_hour"] = df_raw_comments["published_at"].dt.hour
    
                            st.session_state.all_comments_df = df_raw_comments
                            analysis_progress_bar.progress(100, text="ë¶„ì„ ì™„ë£Œ!")
                            st.success(f"ì´ {len(df_raw_comments)}ê°œì˜ ëŒ“ê¸€ì— ëŒ€í•œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        else:
                            st.warning("ëŒ“ê¸€ ë°ì´í„°ì— ìœ íš¨í•œ 'text' ë‚´ìš©ì´ ì—†ì–´ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            st.session_state.all_comments_df = pd.DataFrame()
                        if analysis_status_text is not None: analysis_status_text.empty()
            else:
                st.error("ë¶„ì„í•  ì˜ìƒì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.markdown("---")

    # --- 3. ë¶„ì„ ê²°ê³¼ íƒ­ ì„¹ì…˜ ---
    if not st.session_state.all_comments_df.empty:
        st.header("ëŒ“ê¸€ ë¶„ì„ ê²°ê³¼")
        df_analysis_results = st.session_state.all_comments_df.copy()

        valid_sentiments_global = df_analysis_results["sentiment_labels"].dropna().apply(
            lambda x: [s for s in x if s != 'ì—†ìŒ'] if isinstance(x, list) and x else None
        ).dropna()
        sentiment_counts_global = Counter([label for sublist in valid_sentiments_global for label in sublist])

        valid_disasters_global = df_analysis_results["disaster_labels"].dropna().apply(
            lambda x: x if isinstance(x, list) and x else None
        ).dropna()
        disaster_label_counts_global = Counter([label for sublist in valid_disasters_global for label in sublist])

        tab_titles_list = [
            "ì¢…í•© ìš”ì•½", "ì „ì²´ ê°ì • ë¶„í¬", "ì‹œê°„ëŒ€ë³„ ê°ì •", "ì „ì²´ í‚¤ì›Œë“œ",
            "ì •ë³„ í‚¤ì›Œë“œ", "ì¬ë‚œ ìœ í˜•ë³„ ë¶„ì„", "í•­ëª© ê°„ ë¹„êµ ë¶„ì„"
        ]
        (tab_summary, tab_sentiment_distribution, tab_sentiment_over_time, tab_all_keywords,
        tab_keywords_by_emotion, tab_analysis_by_disaster, tab_comparative_analysis) = st.tabs(tab_titles_list)

        with tab_summary:
            st.subheader("ì¢…í•© ìš”ì•½ ë° ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            total_comments = len(df_analysis_results)
            num_videos = df_analysis_results['video_id'].nunique()
            avg_comments_video = total_comments / num_videos if num_videos > 0 else 0

            sum_col1, sum_col2, sum_col3 = st.columns(3)
            with sum_col1: st.metric("ì´ ë¶„ì„ ëŒ“ê¸€ ìˆ˜", f"{total_comments:,} ê°œ")
            with sum_col2: st.metric("ë¶„ì„ëœ ì˜ìƒ ìˆ˜", f"{num_videos} ê°œ")
            with sum_col3: st.metric("ì˜ìƒë‹¹ í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{avg_comments_video:.1f} ê°œ")

            if st.checkbox("ìˆ˜ì§‘ëœ ì „ì²´ ëŒ“ê¸€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5ê°œ)", key="show_raw_data_summary_tab_v4"): # í‚¤ ë³€ê²½
                display_cols = ['video_title', 'text', 'sentiment_labels', 'disaster_labels', 'published_at', 'like_count']
                existing_display_cols = [col for col in display_cols if col in df_analysis_results.columns]
                st.dataframe(df_analysis_results[existing_display_cols].head())

        with tab_sentiment_distribution:
            st.subheader("ì „ì²´ ëŒ“ê¸€ì˜ ê°ì • ë¶„í¬ (KOTE Multi-label)")
            if sentiment_counts_global:
                df_sentiment_dist = pd.DataFrame(
                    sentiment_counts_global.items(), columns=['sentiment', 'count']
                ).sort_values(by='count', ascending=False)

                # ì›Œë“œ í´ë¼ìš°ë“œëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë¯€ë¡œ, ë°ì´í„°í”„ë ˆì„ì„ ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                sentiment_dict = pd.Series(df_sentiment_dist['count'].values, index=df_sentiment_dist['sentiment']).to_dict()
                
                ui_helpers.create_bar_chart(
                df_sentiment_dist, x_col='sentiment', y_col='count',
                title="ì£¼ìš” ê°ì • ë ˆì´ë¸” ë¹ˆë„ (ìƒìœ„ 15ê°œ)", color_col='sentiment', top_n=15,
                key_suffix="_global_sent_dist_v4" # í‚¤ ë³€ê²½
                )
                
                if st.checkbox("ì „ì²´ ê°ì • ë¹ˆë„ ë°ì´í„° ë³´ê¸°", key="show_all_sentiment_data_dist_tab_v4"): # í‚¤ ë³€ê²½
                    ui_helpers.display_dataframe_with_title(df_sentiment_dist, "ì „ì²´ ê°ì • ë¹ˆë„", "_all_sent_data_v4") # í‚¤ ë³€ê²½
            else:
                st.info("ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ 'ì—†ìŒ' ê°ì •ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.")
        
        df_analysis_results['comment_date'] = df_analysis_results['published_at'].dt.date

        df_analysis_results['published_at'] = pd.to_datetime(df_analysis_results['published_at'])
        df_analysis_results['comment_year'] = df_analysis_results['published_at'].dt.year

        def display_sentiment_trend_chart(df_analysis, sentiment_counts, time_unit, top_n=5, title_suffix="", x_axis_type='full'):
            """
            ì§€ì •ëœ ì‹œê°„ ë‹¨ìœ„(ì‹œê°„, ì—°ë„ ë“±)ì— ë”°ë¥¸ ê°ì • ë³€í™” ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            (Xì¶• ë²”ìœ„ ìë™ ì¡°ì ˆ ê¸°ëŠ¥ ì¶”ê°€)
            """
            
            # ì‹œê°„ ë‹¨ìœ„ì— ë”°ë¥¸ ì„¤ì •(Configuration) ì •ì˜
            configs = {
                'hour': {
                    'col': 'comment_hour', 'title_prefix': 'ì‹œê°„ëŒ€ë³„', 'x_label': 'ëŒ“ê¸€ ì‘ì„± ì‹œê°„',
                    'check_unique': lambda df: True, 'fail_msg': "",
                    'get_ticks': lambda df, axis_type: list(range(24)) if axis_type == 'full' else sorted(df['comment_hour'].unique())
                },
                'year': {
                    'col': 'comment_year', 'title_prefix': 'ì—°ë„ë³„', 'x_label': 'ëŒ“ê¸€ ì‘ì„± ì—°ë„',
                    'check_unique': lambda df: df['comment_year'].nunique() > 1,
                    'fail_msg': "ëª¨ë“  ëŒ“ê¸€ì´ ë™ì¼í•œ ì—°ë„ì— ì‘ì„±ë˜ì–´ ì—°ë„ë³„ ì¶”ì´ ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.",
                    'get_ticks': lambda df, axis_type: sorted(df['comment_year'].unique())
                }
            }
            
            cfg = configs.get(time_unit)
            if not cfg:
                st.error(f"'{time_unit}'ì€(ëŠ”) ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‹œê°„ ë‹¨ìœ„ì…ë‹ˆë‹¤.")
                return

            # [ê°€ë“œ í´ë¡œì¦ˆ] ë¡œì§ (ì´ì „ê³¼ ë™ì¼)
            if cfg['col'] not in df_analysis.columns or not sentiment_counts:
                st.info(f"ë¶„ì„ì— í•„ìš”í•œ ì •ë³´(ê²Œì‹œ {cfg['title_prefix']} ë˜ëŠ” ê°ì •)ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            if not cfg['check_unique'](df_analysis):
                st.info(cfg['fail_msg'])
                return

            # ë°ì´í„° ì „ì²˜ë¦¬ ë° ê°€ê³µ
            df_processed = df_analysis.copy().explode('sentiment_labels').dropna(subset=['sentiment_labels']).query("sentiment_labels != 'ì—†ìŒ'")
            if df_processed.empty:
                st.info(f"{cfg['title_prefix']} ë¶„ì„ì„ ìœ„í•œ ìœ íš¨í•œ ê°ì • ë°ì´í„°ê°€ í¬í•¨ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            # ìƒìœ„ Nê°œ ê°ì • í•„í„°ë§ ë¡œì§
            top_sentiments_df = pd.DataFrame(sentiment_counts.items(), columns=['sentiment', 'count']).sort_values(by='count', ascending=False)
            top_sentiments_list = top_sentiments_df.head(top_n)['sentiment'].tolist()
            if not top_sentiments_list:
                st.info(f"ë¶„ì„ëœ ì£¼ìš” ê°ì •ì´ ì—†ìŠµë‹ˆë‹¤ ({cfg['title_prefix']} ì°¨íŠ¸).")
                return

            # ìµœì¢… ì°¨íŠ¸ ë°ì´í„° ìƒì„±
            chart_df = df_processed.query("sentiment_labels in @top_sentiments_list").groupby([cfg['col'], 'sentiment_labels']).size().reset_index(name='count')
            if chart_df.empty:
                st.info(f"ì„ íƒëœ ì£¼ìš” ê°ì •(ìƒìœ„ {len(top_sentiments_list)}ê°œ)ì— ëŒ€í•œ {cfg['title_prefix']} ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return

            # ------------------ [í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ì‹œì‘] ------------------
            # Xì¶• ëˆˆê¸ˆê³¼ ë²”ìœ„(range)ë¥¼ ì„¤ì •
            x_ticks = cfg['get_ticks'](chart_df, x_axis_type)
            xaxis_range = None # ê¸°ë³¸ê°’ì€ None (ìë™)

            # ì‹œê°„ëŒ€ ë¶„ì„ì´ê³ , xì¶• ìë™ ì„¤ì •ì´ë©°, ë°ì´í„°ê°€ í•˜ë‚˜ì¼ ë•Œ ë²”ìœ„ ìˆ˜ë™ ì§€ì •
            if time_unit == 'hour' and x_axis_type == 'auto' and len(x_ticks) == 1:
                center_hour = x_ticks[0]
                # í•´ë‹¹ ì‹œê°„ì˜ ì¢Œìš° 1ì‹œê°„ ë²”ìœ„ë¥¼ ë³´ì—¬ì£¼ë˜, 0~23ì‹œë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šê²Œ í•¨
                range_min = max(0, center_hour - 1)
                range_max = min(23, center_hour + 1)
                xaxis_range = [range_min, range_max]
            # ------------------- [í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ë] -------------------

            # ì°¨íŠ¸ ìƒì„±
            final_title = f"{cfg['title_prefix']} ì£¼ìš” ê°ì • ë¹ˆë„ (ìƒìœ„ {len(top_sentiments_list)}ê°œ){title_suffix}"
            
            # ui_helpers.create_line_chart í•¨ìˆ˜ê°€ xaxis_range ì¸ìë¥¼ ë°›ì„ ìˆ˜ ìˆì–´ì•¼ í•¨
            ui_helpers.create_line_chart(
                chart_df,
                x_col=cfg['col'], y_col="count", color_col="sentiment_labels",
                title=final_title,
                x_label=cfg['x_label'], y_label="ëŒ“ê¸€ ìˆ˜", color_label="ê°ì • ë ˆì´ë¸”",
                x_tickvals=x_ticks,
                xaxis_range=xaxis_range # ìƒˆë¡œ ì¶”ê°€ëœ ì¸ì
            )


        # ----------------------------------------------------------------------
        # 2. Streamlit UI ë Œë”ë§ (í˜¸ì¶œ ë¶€ë¶„)
        # ----------------------------------------------------------------------
        with tab_sentiment_over_time:
            
            # ==================================================================
            # 1. ì‹œê°„ëŒ€ë³„ ê°ì • ë³€í™” ë¶„ì„ (ë‚ ì§œ ì„ íƒ ê¸°ëŠ¥ ì¶”ê°€)
            # ==================================================================
            st.subheader("ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ê°ì • ë³€í™” (KOTE Multi-label)")

            analysis_mode = st.radio(
                "ë¶„ì„ ë²”ìœ„ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
                ('ì „ì²´ ê¸°ê°„', 'íŠ¹ì • ë‚ ì§œ ì„ íƒ'),
                horizontal=True,
                key='time_analysis_mode'
            )

            if analysis_mode == 'ì „ì²´ ê¸°ê°„':
                # ì „ì²´ ê¸°ê°„ ë¶„ì„ ì‹œì—ëŠ” xì¶•ì„ 0-23ì‹œ ëª¨ë‘ í‘œì‹œ
                display_sentiment_trend_chart(
                    df_analysis_results, 
                    sentiment_counts_global, 
                    time_unit='hour',
                    x_axis_type='full'
                )
            
            elif analysis_mode == 'íŠ¹ì • ë‚ ì§œ ì„ íƒ':
                # ë°ì´í„°ì— ìˆëŠ” ë‚ ì§œì˜ ìµœì†Œ/ìµœëŒ€ê°’ êµ¬í•˜ê¸°
                min_date = df_analysis_results['comment_date'].min()
                max_date = df_analysis_results['comment_date'].max()

                # ë‚ ì§œ ì„ íƒ ìœ„ì ¯
                selected_date = st.date_input(
                    "ë¶„ì„í•  ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
                    value=max_date,
                    min_value=min_date,
                    max_value=max_date,
                    format="YYYY-MM-DD",
                    key='date_selector_for_time'
                )

                if selected_date:
                    # ì„ íƒëœ ë‚ ì§œë¡œ ë°ì´í„° í•„í„°ë§
                    df_filtered_by_date = df_analysis_results[df_analysis_results['comment_date'] == selected_date]

                    if df_filtered_by_date.empty:
                        st.info(f"**{selected_date.strftime('%Y-%m-%d')}**ì—ëŠ” ì‘ì„±ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        # ì„ íƒëœ ë‚ ì§œì˜ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ê°ì • ë¹ˆë„ìˆ˜ ë‹¤ì‹œ ê³„ì‚°
                        filtered_sentiment_counts = (
                            df_filtered_by_date.copy()
                            .explode('sentiment_labels')
                            .dropna(subset=['sentiment_labels'])
                            .query("sentiment_labels != 'ì—†ìŒ'")
                            ['sentiment_labels'].value_counts().to_dict()
                        )
                        
                        # íŠ¹ì • ë‚ ì§œ ë¶„ì„ ì‹œì—ëŠ” xì¶•ì„ ë°ì´í„°ì— ë§ê²Œ ìë™ìœ¼ë¡œ ì„¤ì •
                        display_sentiment_trend_chart(
                            df_filtered_by_date, 
                            filtered_sentiment_counts, 
                            time_unit='hour',
                            title_suffix=f" - {selected_date.strftime('%Y-%m-%d')}",
                            x_axis_type='auto' # xì¶•ì„ ë™ì ìœ¼ë¡œ ì„¤ì •
                        )

            st.divider()

            # ==================================================================
            # 2. ì—°ë„ë³„ ê°ì • ë³€í™” ë¶„ì„ (ê¸°ì¡´ê³¼ ë™ì¼)
            # ==================================================================
            st.subheader("ì—°ë„ë³„ ëŒ“ê¸€ ê°ì • ë³€í™” (KOTE Multi-label)")
            display_sentiment_trend_chart(
                df_analysis_results, 
                sentiment_counts_global, 
                time_unit='year'
            )
        with tab_all_keywords:
            st.subheader("ì£¼ìš” í‚¤ì›Œë“œ (ì „ì²´ ëŒ“ê¸€ì—ì„œ ì¶”ì¶œ)")
            all_comments_text_combined = ""
            if 'text' in df_analysis_results.columns and not df_analysis_results['text'].isnull().all():
                text_series_all = df_analysis_results["text"].astype(str).str.strip().dropna()
                filtered_text_series_all = text_series_all[
                    (text_series_all != 'nan') & (text_series_all != '')
                ]
                all_comments_text_combined = " ".join(filtered_text_series_all)

            if all_comments_text_combined.strip():
                # extract_keywordsê°€ ë¦¬ìŠ¤íŠ¸ [('ë‹¨ì–´', ë¹ˆë„), ...]ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                top_keywords_list = text_analysis_module.extract_keywords(all_comments_text_combined, num_keywords=50)
                
                if top_keywords_list:
                    # ğŸ’¡ [í•´ê²°] ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                    top_keywords_dict = dict(top_keywords_list)
                    
                    # ë³€í™˜ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ì›Œë“œ í´ë¼ìš°ë“œ í•¨ìˆ˜ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
                    ui_helpers.create_wordcloud(
                        top_keywords_dict,
                        title="ì „ì²´ ëŒ“ê¸€ ì£¼ìš” í‚¤ì›Œë“œ ë¹ˆë„" 
                    )
                else:
                    st.info("ì „ì²´ ëŒ“ê¸€ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.info("í‚¤ì›Œë“œ ë¶„ì„ì„ ìœ„í•œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        with tab_keywords_by_emotion:
            st.subheader("íŠ¹ì • ê°ì • ê´€ë ¨ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„")
            available_emotions_for_kw_analysis = sorted(list(sentiment_counts_global.keys()))

            if not available_emotions_for_kw_analysis:
                st.info("í‚¤ì›Œë“œë¥¼ ë¶„ì„í•  ê°ì •ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëŒ“ê¸€ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.")
            else:
                selected_emotion_for_kw = st.selectbox(
                    "í‚¤ì›Œë“œë¥¼ ë¶„ì„í•  ê°ì •ì„ ì„ íƒí•˜ì„¸ìš”:",
                    options=available_emotions_for_kw_analysis,
                    index=0 if available_emotions_for_kw_analysis else None,
                    key="emotion_keyword_selectbox_v4" # í‚¤ ë³€ê²½
                )
                if selected_emotion_for_kw:
                    emotion_specific_comments_df = df_analysis_results[
                        df_analysis_results['sentiment_labels'].apply(
                            lambda x: isinstance(x, list) and selected_emotion_for_kw in x
                        )
                    ]
                    if emotion_specific_comments_df.empty:
                        st.warning(f"'{selected_emotion_for_kw}' ê°ì •ì´ í¬í•¨ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.markdown(f"#### '{selected_emotion_for_kw}' ê°ì • ê´€ë ¨ ì£¼ìš” í‚¤ì›Œë“œ (ëŒ“ê¸€ {len(emotion_specific_comments_df)}ê°œ ëŒ€ìƒ)")
                        emotion_specific_text_combined = ""
                        if 'text' in emotion_specific_comments_df.columns and not emotion_specific_comments_df['text'].isnull().all():
                            text_series_emotion = emotion_specific_comments_df["text"].astype(str).str.strip().dropna()
                            filtered_text_series_emotion = text_series_emotion[
                                (text_series_emotion != 'nan') & (text_series_emotion != '')
                            ]
                            emotion_specific_text_combined = " ".join(filtered_text_series_emotion)

                        if emotion_specific_text_combined.strip():
                            emotion_keywords = text_analysis_module.extract_keywords(emotion_specific_text_combined, num_keywords=15)
                            if emotion_keywords:
                                df_emotion_kws = pd.DataFrame(emotion_keywords, columns=["keyword", "count"])
                                ui_helpers.create_bar_chart(
                                    df_emotion_kws, x_col='count', y_col='keyword', orientation='h',
                                    title=f"'{selected_emotion_for_kw}' ê°ì • ì£¼ìš” í‚¤ì›Œë“œ (ìƒìœ„ 15ê°œ)", top_n=15,
                                    key_suffix=f"_kw_v4_{selected_emotion_for_kw.replace('/', '_').replace(' ', '_')}" # í‚¤ ë³€ê²½
                                )
                                if st.checkbox(f"'{selected_emotion_for_kw}' ê´€ë ¨ ëŒ“ê¸€ ì¼ë¶€ ë³´ê¸°", key=f"show_comments_for_emotion_kw_v4_{selected_emotion_for_kw.replace(' ', '_')}"): # í‚¤ì— ê³µë°± ì œê±°
                                    st.dataframe(emotion_specific_comments_df[['video_title', 'text', 'sentiment_labels']].head(10))
                            else:
                                st.info(f"'{selected_emotion_for_kw}' ê°ì • ê´€ë ¨ ëŒ“ê¸€ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
                        else:
                            st.info(f"'{selected_emotion_for_kw}' ê°ì • ê´€ë ¨ ëŒ“ê¸€ì— ë¶„ì„í•  ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        with tab_analysis_by_disaster:
            st.subheader("ì¬ë‚œ ìœ í˜•ë³„ ì‹¬ì¸µ ë¶„ì„")
            if not disaster_label_counts_global:
                st.info("ëŒ“ê¸€ì—ì„œ ì‹ë³„ëœ ì¬ë‚œ ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                sorted_disaster_types = sorted(list(disaster_label_counts_global.keys()))

                if len(sorted_disaster_types) == 1:
                    disaster_type = sorted_disaster_types[0]
                    st.markdown(f"#### '{disaster_type}' ê´€ë ¨ ëŒ“ê¸€ ë¶„ì„")
                    disaster_specific_df = df_analysis_results[
                        df_analysis_results["disaster_labels"].apply(lambda x: isinstance(x, list) and disaster_type in x)
                    ]
                    if disaster_specific_df.empty:
                        st.write(f"'{disaster_type}' ê´€ë ¨ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        col_dis_sent, col_dis_kw = st.columns(2)
                        with col_dis_sent:
                            st.markdown("##### ê°ì • ë¶„í¬ (KOTE)")
                            dis_valid_sents = disaster_specific_df["sentiment_labels"].dropna().apply(
                                lambda x: [s for s in x if s != 'ì—†ìŒ'] if isinstance(x, list) and x else []
                            ).tolist()
                            dis_valid_sents_flattened = [label for sublist in dis_valid_sents if sublist for label in sublist]
                            if dis_valid_sents_flattened:
                                dis_sent_counts = Counter(dis_valid_sents_flattened)
                                df_dis_sent = pd.DataFrame(dis_sent_counts.items(), columns=['sentiment', 'count']).sort_values(by='count', ascending=False)
                                ui_helpers.create_bar_chart(
                                    df_dis_sent, x_col='sentiment', y_col='count',
                                    title=f"'{disaster_type}' ì£¼ìš” ê°ì • (ìƒìœ„ 10ê°œ)", color_col='sentiment', top_n=10,
                                    key_suffix=f"_sent_v4_{disaster_type.replace(' ', '_')}" # í‚¤ ë³€ê²½
                                )
                            else: st.info("í•´ë‹¹ ì¬ë‚œ ìœ í˜• ê´€ë ¨ ëŒ“ê¸€ì—ì„œ ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        with col_dis_kw:
                            st.markdown("##### ì£¼ìš” í‚¤ì›Œë“œ")
                            dis_text = ""
                            if 'text' in disaster_specific_df.columns and not disaster_specific_df['text'].isnull().all():
                                text_series_dis_single = disaster_specific_df["text"].astype(str).str.strip().dropna()
                                filtered_text_series_dis_single = text_series_dis_single[
                                    (text_series_dis_single != 'nan') & (text_series_dis_single != '')
                                ]
                                dis_text = " ".join(filtered_text_series_dis_single)

                            if dis_text.strip():
                                dis_kws = text_analysis_module.extract_keywords(dis_text, num_keywords=10)
                                if dis_kws:
                                    df_dis_kws = pd.DataFrame(dis_kws, columns=["keyword", "count"])
                                    ui_helpers.create_bar_chart(
                                        df_dis_kws, x_col='keyword', y_col='count',
                                        title=f"ê´€ë ¨ í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ)", top_n=10,
                                        key_suffix=f"_kw_v4_{disaster_type.replace(' ', '_')}" # í‚¤ ë³€ê²½
                                    )
                                else: st.info("ì¶”ì¶œëœ í‚¤ì›Œë“œ ì—†ìŒ")
                            else: st.info("í‚¤ì›Œë“œ ë¶„ì„ì„ ìœ„í•œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ì—†ìŒ")
                        if st.checkbox(f"'{disaster_type}' ê´€ë ¨ ëŒ“ê¸€ ë³´ê¸° ({len(disaster_specific_df)}ê°œ)", key=f"show_comments_single_dis_v4_{disaster_type.replace(' ', '_')}"): # í‚¤ì— ê³µë°± ì œê±°
                            st.dataframe(disaster_specific_df[['video_title', 'text', 'sentiment_labels', 'published_at']].head(20))
                else:
                    disaster_sub_tabs = st.tabs([dt.replace(' ', '_') for dt in sorted_disaster_types]) # íƒ­ ì´ë¦„ì— ê³µë°± ì œê±°
                    for i, disaster_type_in_tab in enumerate(sorted_disaster_types):
                        with disaster_sub_tabs[i]:
                            st.markdown(f"##### '{disaster_type_in_tab}' ê´€ë ¨ ëŒ“ê¸€ ë¶„ì„")
                            disaster_df_for_tab = df_analysis_results[
                                df_analysis_results["disaster_labels"].apply(lambda x: isinstance(x, list) and disaster_type_in_tab in x)
                            ]
                            if disaster_df_for_tab.empty:
                                st.write(f"'{disaster_type_in_tab}' ê´€ë ¨ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                                continue

                            col_tab_sent, col_tab_kw = st.columns(2)
                            with col_tab_sent:
                                st.markdown("###### ê°ì • ë¶„í¬ (KOTE)")
                                tab_valid_sents = disaster_df_for_tab["sentiment_labels"].dropna().apply(
                                    lambda x: [s for s in x if s != 'ì—†ìŒ'] if isinstance(x, list) and x else []
                                ).tolist()
                                tab_valid_sents_flattened = [label for sublist in tab_valid_sents if sublist for label in sublist]
                                if tab_valid_sents_flattened:
                                    tab_sent_counts = Counter(tab_valid_sents_flattened)
                                    df_tab_sent = pd.DataFrame(tab_sent_counts.items(), columns=['sentiment', 'count']).sort_values(by='count', ascending=False)
                                    ui_helpers.create_bar_chart(
                                        df_tab_sent, x_col='sentiment', y_col='count',
                                        title=f"ì£¼ìš” ê°ì • (ìƒìœ„ 10ê°œ)", color_col='sentiment', top_n=10,
                                        key_suffix=f"_sent_tab_v4_{disaster_type_in_tab.replace(' ', '_')}" # í‚¤ ë³€ê²½
                                    )
                                else: st.info("ê°ì • ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
                            with col_tab_kw:
                                st.markdown("###### ì£¼ìš” í‚¤ì›Œë“œ")
                                tab_text = ""
                                if 'text' in disaster_df_for_tab.columns and not disaster_df_for_tab['text'].isnull().all():
                                    text_series_dis_tab = disaster_df_for_tab["text"].astype(str).str.strip().dropna()
                                    filtered_text_series_dis_tab = text_series_dis_tab[
                                        (text_series_dis_tab != 'nan') & (text_series_dis_tab != '')
                                    ]
                                    tab_text = " ".join(filtered_text_series_dis_tab)

                                if tab_text.strip():
                                    tab_kws = text_analysis_module.extract_keywords(tab_text, num_keywords=10)
                                    if tab_kws:
                                        df_tab_kws = pd.DataFrame(tab_kws, columns=["keyword", "count"])
                                        ui_helpers.create_bar_chart(
                                            df_tab_kws, x_col='keyword', y_col='count',
                                            title=f"ê´€ë ¨ í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ)", top_n=10,
                                            key_suffix=f"_kw_tab_v4_{disaster_type_in_tab.replace(' ', '_')}" # í‚¤ ë³€ê²½
                                        )
                                    else: st.info("ì¶”ì¶œëœ í‚¤ì›Œë“œ ì—†ìŒ")
                                else: st.info("í‚¤ì›Œë“œ ë¶„ì„ì„ ìœ„í•œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ì—†ìŒ")

                            if st.checkbox(f"'{disaster_type_in_tab}' ê´€ë ¨ ëŒ“ê¸€ ë³´ê¸° ({len(disaster_df_for_tab)}ê°œ)", key=f"show_tab_comments_v4_{disaster_type_in_tab.replace(' ', '_')}"): # í‚¤ì— ê³µë°± ì œê±°
                                st.dataframe(disaster_df_for_tab[['video_title', 'text', 'sentiment_labels', 'published_at']].head(20))

        # app.py (tab_comparative_analysis ì „ì²´ ì½”ë“œ)

    # ... (ì´ì „ íƒ­ë“¤ ë° í•„ìš”í•œ import, ë³€ìˆ˜ ì„¤ì •ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆë‹¤ê³  ê°€ì •) ...
    # df_analysis_results, st.session_state.selected_video_ids_titles, disaster_label_counts_global ë“±ì´ ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•¨

        # --- íƒ­ 7: í•­ëª© ê°„ ë¹„êµ ë¶„ì„ ---
        with tab_comparative_analysis:
            st.subheader("í•­ëª© ê°„ ë°ì´í„° ë¹„êµ ë¶„ì„")

            comparison_target_type = st.radio(
                "ë¹„êµí•  ëŒ€ìƒì„ ì„ íƒí•˜ì„¸ìš”:",
                ("ì˜ìƒ ê°„ ë¹„êµ", "ì¬ë‚œ ìœ í˜• ê°„ ë¹„êµ"),
                horizontal=True,
                key="main_comparison_target_type_radio_tab7_v6" # í‚¤ ì—…ë°ì´íŠ¸
            )

            # ë¹„êµ UIë¥¼ ì‹¤ì œë¡œ í‘œì‹œí• ì§€ ê²°ì •í•˜ëŠ” í”Œë˜ê·¸
            display_comparison_ui = False
            # ë¹„êµ ë¶„ì„ì— ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ë  ì•„ì´í…œ ëª©ë¡ (ì´ˆê¸°í™”)
            selected_items_for_final_comparison = []


            if comparison_target_type == "ì˜ìƒ ê°„ ë¹„êµ":
                # "1. ì˜ìƒ ê²€ìƒ‰ ë° ë¶„ì„ ì„¤ì •" ë‹¨ê³„ì—ì„œ ì‚¬ìš©ìê°€ ì„ íƒí•˜ì—¬
                # ì‹¤ì œë¡œ ëŒ“ê¸€ ë¶„ì„ì´ ìˆ˜í–‰ëœ ì˜ìƒë“¤ì˜ ì œëª© ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
                # st.session_state.selected_video_ids_titlesëŠ” ë¶„ì„ ì‹œì‘ ì‹œì ì˜ ì„ íƒ ëª©ë¡ì´ë¯€ë¡œ,
                # df_analysis_resultsì— ìˆëŠ” ì‹¤ì œ ë¶„ì„ëœ ì˜ìƒ ì œëª©ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                if 'video_title' in df_analysis_results.columns:
                    analyzed_video_titles_for_comparison_options = sorted(list(df_analysis_results['video_title'].unique()))
                else:
                    analyzed_video_titles_for_comparison_options = [] # ë¶„ì„ ê²°ê³¼ì— video_titleì´ ì—†ëŠ” ê²½ìš°

                num_analyzed_videos = len(analyzed_video_titles_for_comparison_options)

                if num_analyzed_videos < 2:
                    st.info(f"ì˜ìƒ ê°„ ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì˜ìƒì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ ë¶„ì„ëœ ê³ ìœ  ì˜ìƒì€ {num_analyzed_videos}ê°œì…ë‹ˆë‹¤. '1. ì˜ìƒ ê²€ìƒ‰ ë° ë¶„ì„ ì„¤ì •' ë‹¨ê³„ì—ì„œ ë” ë§ì€ ì˜ìƒì„ ì„ íƒí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                else:
                    # min_selections ì¸ì ì—†ì´ multiselect ì‚¬ìš©
                    temp_selected_videos = st.multiselect(
                        f"ë¶„ì„ëœ ì˜ìƒ ì¤‘ ë¹„êµí•  ì˜ìƒì„ ì„ íƒí•˜ì„¸ìš” (í˜„ì¬ {num_analyzed_videos}ê°œ ë¶„ì„ë¨, 2ê°œ ì´ìƒ ì„ íƒ):",
                        options=analyzed_video_titles_for_comparison_options,
                        default=[],
                        key="compare_videos_multiselect_tab7_v6" # í‚¤ ì—…ë°ì´íŠ¸
                    )

                    if not temp_selected_videos:
                        st.info("ìœ„ ëª©ë¡ì—ì„œ ë¹„êµí•  ì˜ìƒì„ 2ê°œ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    elif len(temp_selected_videos) == 1:
                        st.warning(f"1ê°œì˜ ì˜ìƒë§Œ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤ ('{temp_selected_videos[0]}'). ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” 2ê°œ ì´ìƒì˜ ì˜ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    else: # 2ê°œ ì´ìƒ ì„ íƒë¨
                        selected_items_for_final_comparison = temp_selected_videos
                        display_comparison_ui = True # ë¹„êµ UI í‘œì‹œ ì¡°ê±´ ì¶©ì¡±
                
                # --- ì˜ìƒ ê°„ ë¹„êµ UI ë° ë°ì´í„° ì²˜ë¦¬ ë¡œì§ ---
                # --- ì˜ìƒ ê°„ ë¹„êµ UI ë° ë°ì´í„° ì²˜ë¦¬ ë¡œì§ ---
                if display_comparison_ui and selected_items_for_final_comparison:
                    cols_for_comparison = st.columns(len(selected_items_for_final_comparison))

                    for i, video_title_for_comp in enumerate(selected_items_for_final_comparison):
                        with cols_for_comparison[i]:
                            # ì˜ìƒ ì œëª©ì´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë¶€ë§Œ í‘œì‹œ
                            st.markdown(f"##### {video_title_for_comp[:40]}...")
                            
                            item_df_comp = df_analysis_results[df_analysis_results['video_title'] == video_title_for_comp]
                            
                            if item_df_comp.empty:
                                st.write("í•´ë‹¹ ì˜ìƒì— ëŒ€í•œ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                st.markdown("---")
                                continue

                            # 1. ê°ì • ë¶„í¬ ë¹„êµ (ë„ë„› ì°¨íŠ¸ë¡œ ë³€ê²½)
                            st.markdown("**ê°ì • êµ¬ì„±ë¹„**")
                            item_comp_sents = item_df_comp["sentiment_labels"].dropna().apply(
                                lambda x: [s for s in x if s != 'ì—†ìŒ'] if isinstance(x, list) and x else []
                            ).tolist()
                            item_comp_sents_flattened = [label for sublist in item_comp_sents if sublist for label in sublist]
                            
                            if item_comp_sents_flattened:
                                item_comp_sent_counts = Counter(item_comp_sents_flattened)
                                df_item_comp_sent = pd.DataFrame(item_comp_sent_counts.items(), columns=['sentiment', 'count']).sort_values(by='count', ascending=False)
                                
                                # ğŸ’¡ [í•µì‹¬ ë³€ê²½] ë„ë„› ì°¨íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬ (ìƒìœ„ 5ê°œ + ê¸°íƒ€)
                                top_n = 5
                                if len(df_item_comp_sent) > top_n:
                                    top_df = df_item_comp_sent.head(top_n)
                                    others_count = df_item_comp_sent.iloc[top_n:]['count'].sum()
                                    others_df = pd.DataFrame([{'sentiment': 'ê¸°íƒ€', 'count': others_count}])
                                    df_for_donut = pd.concat([top_df, others_df], ignore_index=True)
                                else:
                                    df_for_donut = df_item_comp_sent

                                # ğŸ’¡ [í•µì‹¬ ë³€ê²½] ì˜¬ë°”ë¥¸ ì¸ìë¡œ ë„ë„› ì°¨íŠ¸ í•¨ìˆ˜ í˜¸ì¶œ
                                ui_helpers.create_donut_chart(
                                    df_for_donut,
                                    names_col='sentiment',
                                    values_col='count',
                                    title="ìƒìœ„ ê°ì • ë¶„í¬",
                                    key_suffix=f"_comp_donut_vid_{i}" 
                                )
                            else: 
                                st.write("ê°ì • ë°ì´í„° ì—†ìŒ")

                            # 2. ì£¼ìš” í‚¤ì›Œë“œ ë¹„êµ
                            st.markdown("**ì£¼ìš” í‚¤ì›Œë“œ**")
                            item_comp_text_combined = ""
                            if 'text' in item_df_comp.columns and not item_df_comp['text'].isnull().all():
                                text_series_item_comp = item_df_comp["text"].astype(str).str.strip().dropna()
                                filtered_text_series_item_comp = text_series_item_comp[
                                    (text_series_item_comp != 'nan') & (text_series_item_comp != '')
                                ]
                                item_comp_text_combined = " ".join(filtered_text_series_item_comp)

                            if item_comp_text_combined.strip():
                                item_comp_kws = text_analysis_module.extract_keywords(item_comp_text_combined, num_keywords=5)
                                if item_comp_kws:
                                    keyword_display = "\n".join([f"- {kw} ({count_val})" for kw, count_val in item_comp_kws])
                                    st.markdown(keyword_display)
                                else: 
                                    st.write("ì¶”ì¶œëœ í‚¤ì›Œë“œ ì—†ìŒ")
                            else: 
                                st.write("í‚¤ì›Œë“œ ë¶„ì„ì„ ìœ„í•œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ")
                            st.markdown("---") # ê° ì•„ì´í…œ ë¹„êµ ì»¬ëŸ¼ êµ¬ë¶„

            elif comparison_target_type == "ì¬ë‚œ ìœ í˜• ê°„ ë¹„êµ":
                # ì‹ë³„ëœ ì „ì²´ ì¬ë‚œ ìœ í˜• ëª©ë¡ (disaster_label_counts_globalëŠ” ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
                available_disasters_for_comparison_options = sorted(list(disaster_label_counts_global.keys()))
                num_analyzed_disasters = len(available_disasters_for_comparison_options)

                if num_analyzed_disasters < 2:
                    st.info(f"ì¬ë‚œ ìœ í˜• ê°„ ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì¬ë‚œ ìœ í˜•ì´ ëŒ“ê¸€ì—ì„œ ì‹ë³„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ ì‹ë³„ëœ ê³ ìœ  ì¬ë‚œ ìœ í˜•ì€ {num_analyzed_disasters}ê°œì…ë‹ˆë‹¤.")
                else:
                    # min_selections ì¸ì ì—†ì´ multiselect ì‚¬ìš©
                    temp_selected_disasters = st.multiselect(
                        f"ì‹ë³„ëœ ì¬ë‚œ ìœ í˜• ì¤‘ ë¹„êµí•  ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš” (í˜„ì¬ {num_analyzed_disasters}ê°œ ì‹ë³„ë¨, 2ê°œ ì´ìƒ ì„ íƒ):",
                        options=available_disasters_for_comparison_options,
                        default=[],
                        key="main_compare_disaster_multiselect_tab7_v6" # í‚¤ ì—…ë°ì´íŠ¸
                    )

                    if not temp_selected_disasters:
                        st.info("ìœ„ ëª©ë¡ì—ì„œ ë¹„êµí•  ì¬ë‚œ ìœ í˜•ì„ 2ê°œ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    elif len(temp_selected_disasters) == 1:
                        st.warning(f"1ê°œì˜ ì¬ë‚œ ìœ í˜•ë§Œ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤ ('{temp_selected_disasters[0]}'). ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” 2ê°œ ì´ìƒì˜ ì¬ë‚œ ìœ í˜•ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    else: # 2ê°œ ì´ìƒ ì„ íƒë¨
                        selected_items_for_final_comparison = temp_selected_disasters
                        display_comparison_ui = True # ë¹„êµ UI í‘œì‹œ ì¡°ê±´ ì¶©ì¡±
                
                # --- ì¬ë‚œ ìœ í˜• ê°„ ë¹„êµ UI ë° ë°ì´í„° ì²˜ë¦¬ ë¡œì§ ---
                if display_comparison_ui and selected_items_for_final_comparison:
                    st.markdown(f"#### ì„ íƒëœ ì¬ë‚œ ìœ í˜• ë¹„êµ: **{', '.join(selected_items_for_final_comparison)}**")
                    cols_for_comparison = st.columns(len(selected_items_for_final_comparison))

                    for i, disaster_type_for_comp in enumerate(selected_items_for_final_comparison):
                        with cols_for_comparison[i]:
                            st.markdown(f"##### {disaster_type_for_comp}")
                            # í•´ë‹¹ ì¬ë‚œ ìœ í˜•ì„ í¬í•¨í•˜ëŠ” ëŒ“ê¸€ í•„í„°ë§
                            item_df_comp = df_analysis_results[
                                df_analysis_results["disaster_labels"].apply(lambda x: isinstance(x, list) and disaster_type_for_comp in x)
                            ]
                            if item_df_comp.empty:
                                st.write("í•´ë‹¹ ì¬ë‚œ ìœ í˜•ì— ëŒ€í•œ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                st.markdown("---")
                                continue

                            # 1. ê°ì • ë¶„í¬ ë¹„êµ
                            st.markdown("**ê°ì • ë¶„í¬**")
                            item_comp_sents = item_df_comp["sentiment_labels"].dropna().apply(
                                lambda x: [s for s in x if s != 'ì—†ìŒ'] if isinstance(x, list) and x else []
                            ).tolist()
                            item_comp_sents_flattened = [label for sublist in item_comp_sents if sublist for label in sublist]

                            if item_comp_sents_flattened:
                                item_comp_sent_counts = Counter(item_comp_sents_flattened)
                                df_item_comp_sent = pd.DataFrame(item_comp_sent_counts.items(), columns=['sentiment', 'count']).sort_values(by='count', ascending=False)
                                ui_helpers.create_bar_chart(
                                    df_item_comp_sent, x_col='sentiment', y_col='count',
                                    title="ìƒìœ„ 5ê°œ ê°ì •", color_col='sentiment', top_n=5,
                                    key_suffix=f"_comp_sent_dis_tab7_v6_{disaster_type_for_comp.replace(' ', '_')}_{i}" # í‚¤ ì—…ë°ì´íŠ¸
                                )
                            else: 
                                st.write("ê°ì • ë°ì´í„° ì—†ìŒ")

                            # 2. ì£¼ìš” í‚¤ì›Œë“œ ë¹„êµ
                            st.markdown("**ì£¼ìš” í‚¤ì›Œë“œ**")
                            item_comp_text_combined = ""
                            if 'text' in item_df_comp.columns and not item_df_comp['text'].isnull().all():
                                text_series_item_comp = item_df_comp["text"].astype(str).str.strip().dropna()
                                filtered_text_series_item_comp = text_series_item_comp[
                                    (text_series_item_comp != 'nan') & (text_series_item_comp != '')
                                ]
                                item_comp_text_combined = " ".join(filtered_text_series_item_comp)
                            
                            if item_comp_text_combined.strip():
                                item_comp_kws = text_analysis_module.extract_keywords(item_comp_text_combined, num_keywords=5)
                                if item_comp_kws:
                                    keyword_display = "\n".join([f"- {kw} ({count_val})" for kw, count_val in item_comp_kws])
                                    st.markdown(keyword_display)
                                else: 
                                    st.write("ì¶”ì¶œëœ í‚¤ì›Œë“œ ì—†ìŒ")
                            else: 
                                st.write("í‚¤ì›Œë“œ ë¶„ì„ì„ ìœ„í•œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ")
                            st.markdown("---") # ê° ì•„ì´í…œ ë¹„êµ ì»¬ëŸ¼ êµ¬ë¶„

    elif st.session_state.get('main_analyze_button_clicked') and st.session_state.all_comments_df.empty :
        st.warning("ëŒ“ê¸€ ìˆ˜ì§‘/ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒë‹¨ ì„¤ì •ì„ í™•ì¸í•˜ê³  'ì„ íƒëœ ì˜ìƒ ëŒ“ê¸€ ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ë‹¤ì‹œ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    elif not st.session_state.get('main_search_button_clicked') and not st.session_state.get('main_analyze_button_clicked'):
        st.info("í˜ì´ì§€ ìƒë‹¨ì—ì„œ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê³  ì˜ìƒì„ ê²€ìƒ‰í•œ í›„, ë¶„ì„í•  ì˜ìƒì„ ì„ íƒí•˜ê³  ëŒ“ê¸€ ë¶„ì„ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.")

    st.markdown("---")
    st.caption("YouTube ì¬ë‚œ ì˜ìƒ ëŒ“ê¸€ ë¶„ì„ê¸° | KOTE ê°ì„± ë¶„ì„ ëª¨ë¸ í™œìš©")
    st.caption("API ì‚¬ìš©ëŸ‰ì—ëŠ” ì œí•œì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [Google Cloud Console](https://console.cloud.google.com/apis/dashboard)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")